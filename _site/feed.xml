<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="www.nonlinear.nyc/feed.xml" rel="self" type="application/atom+xml" /><link href="www.nonlinear.nyc/" rel="alternate" type="text/html" /><updated>2020-03-21T21:01:31-04:00</updated><id>www.nonlinear.nyc/feed.xml</id><title type="html">nonlinear</title><subtitle>The anagram of my name is anarchist fool</subtitle><entry><title type="html">Kichari</title><link href="www.nonlinear.nyc/recipe/kichari/" rel="alternate" type="text/html" title="Kichari" /><published>2020-01-29T18:03:16-05:00</published><updated>2020-01-29T18:03:16-05:00</updated><id>www.nonlinear.nyc/recipe/kichari</id><content type="html" xml:base="www.nonlinear.nyc/recipe/kichari/">&lt;ul&gt;
  &lt;li&gt;6 cups water&lt;/li&gt;
  &lt;li&gt;1 cup basmati rice&lt;/li&gt;
  &lt;li&gt;1/2 cup yellow split mung dal (soaked for 1 hour or more)&lt;/li&gt;
  &lt;li&gt;pinch of asafetida (hing) powder&lt;/li&gt;
  &lt;li&gt;1 tbsp everyday savory spice mix&lt;/li&gt;
  &lt;li&gt;2 cups vegetables … coarsely chopped into 1/2 in cubes, leafy greens also coarsely chopped into strips&lt;/li&gt;
  &lt;li&gt;1/2-1 tbsp salt&lt;/li&gt;
  &lt;li&gt;fresh cilantro&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">6 cups water 1 cup basmati rice 1/2 cup yellow split mung dal (soaked for 1 hour or more) pinch of asafetida (hing) powder 1 tbsp everyday savory spice mix 2 cups vegetables … coarsely chopped into 1/2 in cubes, leafy greens also coarsely chopped into strips 1/2-1 tbsp salt fresh cilantro</summary></entry><entry><title type="html">Voices from the Gang Database</title><link href="www.nonlinear.nyc/study/gang-database/" rel="alternate" type="text/html" title="Voices from the Gang Database" /><published>2020-01-29T18:03:16-05:00</published><updated>2020-01-29T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/gang-database</id><content type="html" xml:base="www.nonlinear.nyc/study/gang-database/">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stopspying.org/&quot;&gt;STOP&lt;/a&gt;: Surveillance Technology Oversight Project&lt;/li&gt;
  &lt;li&gt;?&lt;/li&gt;
  &lt;li&gt;Victor Dempsey : &lt;a href=&quot;https://www.legalaidnyc.org/stories/providing-essential-legal-services-in-the-community-justice-unit/&quot;&gt;The Legal Aid Society&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Alex Vitale: &lt;a href=&quot;https://policingandjustice.squarespace.com/&quot;&gt;Policing &amp;amp; Social Justice Project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://static1.squarespace.com/static/5de981188ae1bf14a94410f5/t/5df14904887d561d6cc9455e/1576093963895/2019+New+York+City+Gang+Policing+Report+-+FINAL%29.pdf&quot;&gt;GANG TAKEDOWNS IN THE DE BLASIO ERA:The Dangers of  ‘Precision Policing’ (PDF)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Dense social networks&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">STOP: Surveillance Technology Oversight Project ? Victor Dempsey : The Legal Aid Society Alex Vitale: Policing &amp;amp; Social Justice Project</summary></entry><entry><title type="html">Krita Shortcuts</title><link href="www.nonlinear.nyc/study/krita/" rel="alternate" type="text/html" title="Krita Shortcuts" /><published>2020-01-15T18:03:16-05:00</published><updated>2020-01-15T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/krita</id><content type="html" xml:base="www.nonlinear.nyc/study/krita/">&lt;p&gt;First off, use popup tool for most anything. Find trigger, and learn usage.&lt;/p&gt;

&lt;p&gt;control-shift-F full screen&lt;/p&gt;

&lt;p&gt;For anything popup tool bar DOESN’T cover:&lt;/p&gt;

&lt;p&gt;Alt-UP Activate next layer (was pgup)
Alt-DOWN Activate previous layer (was pgdn)
Control-Shift J cut selection to new layer&lt;/p&gt;

&lt;p&gt;Control shift A deselect&lt;/p&gt;

&lt;p&gt;color Picker
Brushes
outLine selection tool&lt;/p&gt;

&lt;p&gt;Brush Blend Modes Alt-shift-(capitalized letter)&lt;/p&gt;

&lt;p&gt;behind Q
cleaR
Color
color Burn
color Dodge
darKen
diffErence
dIssolve
eXclusion
Hard light
hard mix L
hard overlay P
hUe
liGhten
lineAr burn
linear dodge W
linear light J
luminosity Y
Multiply
Normal
pin light Z
saTuration
Screen
soFt light
Vivid light&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;reorganize per function, based on video&lt;/li&gt;
  &lt;li&gt;add video link&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;control-L levels
control-U HVS adjustment&lt;/p&gt;

&lt;p&gt;layer&lt;/p&gt;

&lt;p&gt;shift-del remove layer
f2 rename current layer&lt;/p&gt;

&lt;p&gt;brush size&lt;/p&gt;

&lt;p&gt;[ decrease
] increase&lt;/p&gt;

&lt;p&gt;G gradient
V move tool&lt;/p&gt;

&lt;p&gt;control-alt-C resize canvas&lt;/p&gt;

&lt;p&gt;zoom&lt;/p&gt;

&lt;p&gt;control + zoom in
control - zoom out&lt;/p&gt;</content><author><name></name></author><summary type="html">First off, use popup tool for most anything. Find trigger, and learn usage. control-shift-F full screen For anything popup tool bar DOESN’T cover: Alt-UP Activate next layer (was pgup) Alt-DOWN Activate previous layer (was pgdn) Control-Shift J cut selection to new layer Control shift A deselect color Picker Brushes outLine selection tool Brush Blend Modes Alt-shift-(capitalized letter) behind Q cleaR Color color Burn color Dodge darKen diffErence dIssolve eXclusion Hard light hard mix L hard overlay P hUe liGhten lineAr burn linear dodge W linear light J luminosity Y Multiply Normal pin light Z saTuration Screen soFt light Vivid light reorganize per function, based on video add video link control-L levels control-U HVS adjustment layer shift-del remove layer f2 rename current layer brush size [ decrease ] increase G gradient V move tool control-alt-C resize canvas zoom control + zoom in control - zoom out</summary></entry><entry><title type="html">Animation Primer</title><link href="www.nonlinear.nyc/study/animation/" rel="alternate" type="text/html" title="Animation Primer" /><published>2020-01-15T18:03:16-05:00</published><updated>2020-01-15T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/animation</id><content type="html" xml:base="www.nonlinear.nyc/study/animation/">&lt;h2 id=&quot;duration&quot;&gt;duration&lt;/h2&gt;

&lt;p&gt;Specifies how many seconds or milliseconds an animation takes to complete one cycle. Default 0&lt;/p&gt;

&lt;h2 id=&quot;delay&quot;&gt;delay&lt;/h2&gt;

&lt;p&gt;Specifies when the animation will start. Default 0&lt;/p&gt;

&lt;h2 id=&quot;function&quot;&gt;function&lt;/h2&gt;

&lt;p&gt;Describes how the animation will progress over one cycle of its duration. Default “ease”&lt;/p&gt;

&lt;h2 id=&quot;count&quot;&gt;count&lt;/h2&gt;

&lt;p&gt;Specifies the number of times an animation is played. Default 1&lt;/p&gt;

&lt;h2 id=&quot;fill-mode&quot;&gt;fill-mode&lt;/h2&gt;

&lt;p&gt;Specifies if the effects of an animation are before the animation starts and after it ends.&lt;/p&gt;

&lt;h2 id=&quot;visibility&quot;&gt;visibility&lt;/h2&gt;

&lt;p&gt;Determines whether or not a transformed element is visible when it is not facing the screen.&lt;/p&gt;</content><author><name></name></author><summary type="html">duration Specifies how many seconds or milliseconds an animation takes to complete one cycle. Default 0 delay Specifies when the animation will start. Default 0 function Describes how the animation will progress over one cycle of its duration. Default “ease” count Specifies the number of times an animation is played. Default 1 fill-mode Specifies if the effects of an animation are before the animation starts and after it ends. visibility Determines whether or not a transformed element is visible when it is not facing the screen.</summary></entry><entry><title type="html">computational guidance systems</title><link href="www.nonlinear.nyc/study/computational-guidance-systems/" rel="alternate" type="text/html" title="computational guidance systems" /><published>2020-01-15T18:03:16-05:00</published><updated>2020-01-15T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/computational-guidance-systems</id><content type="html" xml:base="www.nonlinear.nyc/study/computational-guidance-systems/">&lt;ul&gt;
  &lt;li&gt;Robb Beal&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Machine Learning&lt;/th&gt;
      &lt;th&gt;Optimization&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;prediction, inference, learn from data&lt;/td&gt;
      &lt;td&gt;find max/min, human-in-the-loop, evaluation&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Machine Learning as molecular gastronomy&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;BayesOpt&lt;/th&gt;
      &lt;th&gt;Combinatorial Optimization&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;max/min&lt;/td&gt;
      &lt;td&gt;Interactive: the optimization happens in using the product&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;bayesopt&quot;&gt;BayesOpt&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;variables, what min and max&lt;/li&gt;
  &lt;li&gt;ask optmizer, given bounds above&lt;/li&gt;
  &lt;li&gt;have users evaluate the UI thru representative task (observation)&lt;/li&gt;
  &lt;li&gt;feed value metric of observation, back to 3&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;distance, delay, decay, etc (metrics)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;distance, delay, decay&lt;/li&gt;
      &lt;li&gt;brightness, contrast, saturation, color balance (3)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;optimal state&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Refining Visual Designs: 6 iterations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Humans are good at comparative judging&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;cultural considerations: if they dont share a culture, human-in-the-middle doesnt help.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3-20 inependent design dimensions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;10-1000 of human observations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;diamond chart (definition/execution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a/b test dont tell a lot about optimal design dimension&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=17851964&quot;&gt;google vizier&lt;/a&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2020-01-15 at 7.36.55 PM.png&quot; alt=&quot;Screen Shot 2020-01-15 at 7.36.55 PM&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;https://sigopt.com/&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;https://prowler.io&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;https://sheffieldml.github.io/GPyOpt/&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.joelsimon.net/evo_floorplans.html&quot;&gt;Joel Simon&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Oculus Connect 6: Introducing Hand Tracking on Oculus Quest, Facebook Horizon, and More&lt;/td&gt;
          &lt;td&gt;Oculus](https://www.oculus.com/blog/oculus-connect-6-introducing-hand-tracking-on-oculus-quest-facebook-horizon-and-more/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;ul&gt;
      &lt;li&gt;gesture recognition&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recommendations&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;visual metaphors:&lt;/strong&gt; images that convey a message symbolically
    &lt;ul&gt;
      &lt;li&gt;are both object identifiable?&lt;/li&gt;
      &lt;li&gt;are they integrated?&lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;[VisiBlends, a New Approach to Disrupt Visual Messaging&lt;/td&gt;
              &lt;td&gt;Columbia Engineering](https://engineering.columbia.edu/press-releases/lydia-chilton-visiblends)&lt;img src=&quot;/assets/images/talks/chilton-system_diagram_half_iterate_v2-1600.jpg&quot; alt=&quot;chilton-system_diagram_half_iterate_v2-1600&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;geometric shape matching&lt;/li&gt;
      &lt;li&gt;SymbolFinder: &lt;a href=&quot;https://www.slideshare.net/hmslydia/ai-tools-for-creative-work&quot;&gt;AI Tools for Creative Work&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2020-01-15 at 7.48.32 PM.png&quot; alt=&quot;Screen Shot 2020-01-15 at 7.48.32 PM&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.cs.columbia.edu/~chilton/web/my_publications/VisiFit_CHI_2020_submission.pdf&quot;&gt;VisiFit: AI Tools to Iteratively Improve Visual Blends&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Noun to noun combinations. noun to verb combinations&lt;/li&gt;
      &lt;li&gt;Lydia Chilton!!!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data science for business&lt;/li&gt;
  &lt;li&gt;computational interaction&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pair.withgoogle.com/&quot;&gt;People + AI Guidebook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;School on Computational Interation&lt;/li&gt;
  &lt;li&gt;Machine Learning &amp;amp; User Experience (MLUX) - meetup, SF&lt;/li&gt;
  &lt;li&gt;Sentiment, etc: Multi-objective, trade-off&lt;/li&gt;
  &lt;li&gt;how do you pick the right variables?&lt;/li&gt;
  &lt;li&gt;design skills
    &lt;ul&gt;
      &lt;li&gt;contingency design: what to do when it fails?&lt;/li&gt;
      &lt;li&gt;defining the variables that matter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;personalized interfaces, after a couple of uses, struggle, adaptation&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Robb Beal</summary></entry><entry><title type="html">figma config</title><link href="www.nonlinear.nyc/study/figma-config/" rel="alternate" type="text/html" title="figma config" /><published>2020-01-15T18:03:16-05:00</published><updated>2020-01-15T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/figma-config</id><content type="html" xml:base="www.nonlinear.nyc/study/figma-config/">&lt;ul&gt;
  &lt;li&gt;“Community”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;morning-talks&quot;&gt;Morning Talks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Dylan Field&lt;/li&gt;
  &lt;li&gt;Discover, duplicate, remix&lt;/li&gt;
  &lt;li&gt;Craig Mod&lt;/li&gt;
  &lt;li&gt;Devon Zuegel
    &lt;ul&gt;
      &lt;li&gt;land capture&lt;/li&gt;
      &lt;li&gt;virtuous cycle&lt;/li&gt;
      &lt;li&gt;Congestion pricing (sponsor to change priority)&lt;/li&gt;
      &lt;li&gt;stronger signal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-systems&quot;&gt;Design Systems&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Elements, interconnections and purpose&lt;/li&gt;
  &lt;li&gt;towards a northstar&lt;/li&gt;
  &lt;li&gt;Elements suffer biggest change&lt;/li&gt;
  &lt;li&gt;communicate that change is here to stay&lt;/li&gt;
  &lt;li&gt;communicate the purpose&lt;/li&gt;
  &lt;li&gt;office hours: peer in users&lt;/li&gt;
  &lt;li&gt;Material design dos and donts&lt;/li&gt;
  &lt;li&gt;desirable futures with less side effects&lt;/li&gt;
  &lt;li&gt;documentation: synonyms, why, others, States, variations&lt;/li&gt;
  &lt;li&gt;prepare for the future&lt;/li&gt;
  &lt;li&gt;subsystem frameworks&lt;/li&gt;
  &lt;li&gt;“speed over certainty” example of principles&lt;/li&gt;
  &lt;li&gt;principles create rationale
    &lt;ul&gt;
      &lt;li&gt;transparency trumps complexity&lt;/li&gt;
      &lt;li&gt;opinionated yet flexible&lt;/li&gt;
      &lt;li&gt;purpose out of behavior&lt;/li&gt;
      &lt;li&gt;bend but never break&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;illustration-systems&quot;&gt;Illustration systems&lt;/h3&gt;

&lt;p&gt;Bonnie&lt;/p&gt;

&lt;p&gt;set up to successfuly replicate&lt;/p&gt;

&lt;p&gt;diversity is more form than race&lt;/p&gt;

&lt;p&gt;diversity should evoke belonging&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;build your illustration library&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;audit
&lt;img src=&quot;/assets/images/talks/IMG_8189.jpeg&quot; alt=&quot;IMG_8189&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;illustrations&lt;/li&gt;
          &lt;li&gt;key messages&lt;/li&gt;
          &lt;li&gt;Audiences (marketing, product, b2b, b2c, international)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;design
&lt;img src=&quot;/assets/images/talks/IMG_8190.jpeg&quot; alt=&quot;IMG_8190&quot; /&gt;
&lt;img src=&quot;/assets/images/talks/IMG_8191.jpeg&quot; alt=&quot;IMG_8191&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;organize
&lt;img src=&quot;/assets/images/talks/IMG_8192.jpeg&quot; alt=&quot;IMG_8192&quot; /&gt;
&lt;img src=&quot;/assets/images/talks/IMG_8193.jpeg&quot; alt=&quot;IMG_8193&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;produce
&lt;img src=&quot;/assets/images/talks/IMG_8194.jpeg&quot; alt=&quot;IMG_8194&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;socialize
&lt;img src=&quot;/assets/images/talks/IMG_8195.jpeg&quot; alt=&quot;IMG_8195&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;icons represent concepts not UI (because UI changes)&lt;/p&gt;

    &lt;p&gt;tagging elements in figma&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">“Community”</summary></entry><entry><title type="html">Open CV</title><link href="www.nonlinear.nyc/study/open-cv/" rel="alternate" type="text/html" title="Open CV" /><published>2020-01-15T18:03:16-05:00</published><updated>2020-01-15T18:03:16-05:00</updated><id>www.nonlinear.nyc/study/open-cv</id><content type="html" xml:base="www.nonlinear.nyc/study/open-cv/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Viola-Jones Algorithm (grayscale, find a nose, box travels thru the image)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;“Rapid Object Detection using a Boosted Cascade of Simple Features”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Haar-like Features&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;edge, line, four rectangle&lt;/li&gt;
      &lt;li&gt;greater than 0.3&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 10.38.31 AM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 10.38.31 AM&quot; style=&quot;zoom:50%; padding: 50px 0&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;h2 id=&quot;integral-image&quot;&gt;Integral Image&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;precalculates sums, so each area can be found with just 4 operations&lt;/li&gt;
      &lt;li&gt;makes raster vector (scale-free)&lt;/li&gt;
      &lt;li&gt;as long as it’s rectangles&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 1.16.02 PM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 1.16.02 PM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 1.16.16 PM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 1.16.16 PM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 1.18.23 PM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 1.18.23 PM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 1.18.47 PM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 1.18.47 PM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/talks/Screen Shot 2019-12-10 at 1.18.59 PM.png&quot; alt=&quot;Screen Shot 2019-12-10 at 1.18.59 PM&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-classifiers&quot;&gt;Training Classifiers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;when training, scale the image DOWN&lt;/li&gt;
  &lt;li&gt;mirror images&lt;/li&gt;
  &lt;li&gt;non- images (same size, WITHOUT training to spot), for false positives, labeled&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/3766402_General_framework_for_object_detection&quot;&gt;General framework for object detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adaboost&quot;&gt;Adaboost&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Adaptative Boosting&lt;/li&gt;
  &lt;li&gt;weak classifier (over 50%)&lt;/li&gt;
  &lt;li&gt;strong classifier&lt;/li&gt;
  &lt;li&gt;ensemble method (a lot of weak classifiers make a strong classifier)&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">The Viola-Jones Algorithm (grayscale, find a nose, box travels thru the image)</summary></entry></feed>